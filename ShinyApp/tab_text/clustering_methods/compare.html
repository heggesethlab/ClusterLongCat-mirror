

<div class = "comparisonTable">
<table>
<colgroup>
<col width="2%"></col>
<col width="25%"></col>
<col width="42%"></col>
<col width="28%"></col>
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Descriptions</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Partitioning Around Medoids</td>
<td>A standard clustering approach where the primary goal is to minimize the average dissimilarity of observations to their nearby observations.</td>
<td>Easy to implement; observations have the freedom to change clusters when centroids are recomputed.</td>
<td>The number of clusters k has to be pre-determined.</td>
</tr>
<tr class="even">
<td>Agglomerative Hierarchical Clustering</td>
<td>A clustering algorithm where each data point starts as an individual cluster. At each iteration, similar clusters merge with each other until one or K clusters are formed. Clustering hierarchy is reflected via dendograms.</td>
<td>No need to assume a certain number of clusters; encodes a hierarchy of clusters.</td>
<td>Have to pick a type of linkage dependent on the context.</td>
</tr>
<tr class="odd">
<td>Mixture of Markov Models</td>
<td>Mixture of Markov Models assumes that each observation follows one of the k distinct Markov chains.</td>
<td>Mixtures of Markov models provide an intuitive and clear approach to clustering categorical time series data in contrast to the uncertainty involed in choosing a distance measure of dissimalirty based clustering techiniques.</td>
<td>A mixture of Markov models assumes that there are several distinct Markov chains and that each subject follows one of these Markov models. Often times that Markov assumption, that each event is conditioned only on the previous event, is too strict, and allowing higher order dependences grows the number of parameters quickly. These methods are also computationally intensive.</td>
</tr>
<tr class="even">
<td>MLE</td>
<td>A form of estimating the parameters of a mixture of Markov models using maximum likelihood estimation (MLE) through the expectation maximization algorithm. Implemented using the seqHMM package.</td>
<td>This method fits cleanly into the frequentist framework that much statistical analyis is done in. Itâ€™s outcomes can also be easily compared to other models fitted using MLE with tools such as AIC and BIC.</td>
<td>Inference relies on asymptotic approximation based on model parameters. Method may not find absolute maximum on likelihood space. Method is computationally intensive.</td>
</tr>
<tr class="odd">
<td>Bayesian Estimation</td>
<td>A form of estimating the parameters of a mixture of Markov models using Bayesian estimation. Here we assume a dirichlet prior that is shared between all clusters on each row of the transition matrix. Then, using an MCMC, the parameters of the cluster specific transition matrices are estimated.</td>
<td>Bayesian estimation provides a natural way of combing prior knowledge with data and interpretable inferences without relying on asymptotic approximation. Using Bayesian estimation gets around many drawbacks of MLE, such as if there are no observed transitions between one state and another the model will predict that it is impossible to make that transition. We can also take into account the fact that for many data sets assumed to be modeled as Markov chains that probability of staying in a state is much higher than the probability of transitioning. This can be added into the model by setting the priors on the transition matrices.</td>
<td>It can be difficult to tell whether or not the MCMC has been run for long enough or to troubleshoot it. If stopped too early, the model will produce misleading results. Additionally it can be difficult to compare models built off of Bayesian estimation to those built by maximizing likelihood as the theoretical background for many model comparision measures like the AIC assume that the method is maximizing likelihood.</td>
</tr>
<tr class="even">
<td>Dirichlet Multinomial Clustering</td>
<td>This is an extension of a mixture of Markov models which allows for each subject to have an individual transition matrix which deviates from the cluster specific transition matrix. This is allowed by allowing each cluster to be modeled by a dirichlet distribution, whose parameters are modeled by a negative multinomial distribution. These parameters are then estimated using an MCMC.</td>
<td>This methods allows us to capture within the model the heterogeneity of members of a cluster. This is often a better fit for real data as the assumption that every member of a cluster is perfectly modeled by the cluster specific Markov chain is unlikely to be real.</td>
<td>This model is computationally intensive, and often has a worse performing MCMC than the standard mixture of Markov models. Additionally, the extra parameters can be difficult to interpret and may not add meaningful information to the analysis.</td>
</tr>
</tbody>
</table>
</div>
